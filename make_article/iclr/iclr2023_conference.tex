\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference, times}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

% --- Custom formatting --- 
% images
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.8}
\renewcommand{\textfraction}{0.1}
\setlength{\floatsep}{5pt plus 2pt minus 2pt}
\setlength{\textfloatsep}{5pt plus 2pt minus 2pt}
\setlength{\intextsep}{5pt plus 2pt minus 2pt}
\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

% text
\usepackage{titlesec}

\titlespacing*{\section}
{0pt}{.2cm}{.1cm}
\titlespacing*{\subsection}
{0pt}{.1cm}{.1cm}

% --- Default packages --- 
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{hyperref}
\usepackage{url}

% --- Custom packages --- 
% images
\usepackage{graphicx}
\usepackage{float}
\usepackage{placeins}
\usepackage{wrapfig}
\usepackage{subcaption}

% psudeocode
\usepackage{algorithm}
\usepackage{algpseudocode}
\def\NoNumber#1{{\def\alglinenumber##1{}\State #1}\addtocounter{ALG@line}{-1}}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}

\newcommand{\yli}[1]{{\color{cyan}#1}}
\newcommand{\mh}[1]{{\color{red}#1}}

\title{
    Trusted Aggregation (TAG): Model Filtering Backdoor Defense In Federated Learning
}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{%
    Joseph Lavond \& Yao Li  \\
    Department of Statistics and Operations Research \\
    University of North Carolina at Chapel Hill \\
    Chapel Hill, NC 27514, USA \\
    \texttt{\{jlavond, yaoli\}@unc.email.edu} \\
    %
    \And
    Minhao Cheng \\
    Computer Science and Engineering \\
    Hong Kong University of Science and Technology \\
    Clear Water Bay, Hong Kong \\
    \texttt{minhaocheng@ust.hk}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
Federated Learning is a framework for training machine learning models from multiple local data sets without access to the data. A shared model is jointly learned through an interactive process between server and clients that combines locally learned model gradients or weights. However, the lack of data transparency naturally raises concerns about model security. Recently, several state-of-the-art backdoor attacks have been proposed, which achieve high attack success rates while simultaneously being difficult to detect, leading to compromised federated learning models. In this paper, motivated by differences in the output layer distribution between models trained with and without the presence of backdoor attacks, we propose a defense method that can prevent backdoor attacks from influencing the model while maintaining the accuracy of the original classification task.
\end{abstract}

\section{Introduction}

Federated learning (FL) is a potential solution to constructing a machine learning model from several local data sources that cannot be exchanged or aggregated. As mentioned in~\cite{fed-learn}, these restrictions are essential in areas where data privacy or security is critical, including but not limited to healthcare.  Also, FL is valuable for companies that shift computing workloads to local devices. Furthermore, these local data sets are not required to be independent and identically distributed. Hence, a shared robust global model is desirable and, in many cases, cannot be produced without some form of collaborative learning. Under the FL setting, local entities (clients) submit their locally learned model gradients and weights to be intelligently combined by some centralized entity (server) to create a shared and robust machine learning model.

Concerns have arisen that the lack of control or knowledge regarding the local training procedure could allow a user, with malicious intent, to create an update that compromises the global model for all participating clients. An example of such harm is a backdoor attack,  where the malicious users try to get the global model to associate a given manipulation of the input data, known as a trigger, with a particular outcome. Some methods~\cite{kurita2020weight,qi2020onion,li2021backdoor} have been proposed to detect the triggers in the training data to defend against backdoor attacks. However, in FL, as only the resulting model gradients or weights are communicated back, such methods cannot be applied to defend against backdoor attacks. Furthermore, since the model update in FL assumes no access to all clients' data, there is less information available to help detect and prevent such malicious intent. Thus backdoor attacks may be easier to perform and harder to detect in FL.  Furthermore, current robust aggregation methods~\cite{trim-mean} fail to prevent even mild backdoor attacks.

In this paper, we first find that the output layer distributions of malicious users are very different from that of benign users. Specifically, there exists a discernible difference between malicious and benign user distributions for the target label class. Therefore, we can leverage this difference to detect backdoor attacks. Figure~\ref{fig: motivation} shows a model with different estimated distributions for the target class depending on whether or not that model has been backdoor attacked. 

Motivated by the finding that the output layer distributions of a model with and without a backdoor are different, we propose distributional differences between the output layers of returning user models and a known clean model to identify malicious updates. The proposed method is effective against multiple state-of-the-art backdoor attacks at different strength levels. Even in the unreasonable setting where 40\% of the clients are malicious for each update, we greatly delay the success of the backdoor attack, outperforming current robust aggregation methods.  In the experiment section, we demonstrate our method's ability on several data sets to prevent backdoor attacks.  The method performs well even when the attack happens every round starting at the beginning of the process. Furthermore, our method does not affect the performance of the global model on clean data, resulting in no decrease and even increases in the accuracy of the original classification task.

%
\section{Related Work}

\paragraph{Federated Learning.} Federated learning (FL) is an emerging machine learning paradigm that has seen great success in many fields~\cite{ryffel2018generic,hard2018federated,bonawitz2019towards}. At a high level, FL is an iterative procedure involving rounds of model improvement until it meets some criteria. These rounds send the global model to users and select a subset of users to update the global model. Then those chosen users train their local copy of the model, and their resulting models are communicated back and aggregated to create a new global model. Typically, the final local model's gradients or weights are transmitted back to ensure data privacy. Popular aggregation methods of FL include FedAvg~\cite{fedavg}, Median~\cite{yin2018byzantine} and Trim-mean~\cite{yin2018byzantine}.

\paragraph{Backdoor Attack.} Recently, several backdoor attacks have been proposed to take advantage of the FL setting. In~\cite{dba}, the authors show that the multiple-user nature of FL can be exploitable to make more potent and lasting backdoor attacks. By distributing the backdoor trigger across a few malicious users, they could make the global model exhibit the desired behavior at higher rates and for many iterations after the attack had concluded. We will show our threshold's effectiveness in even more potent attack settings than in their original paper. 

A recent work~\cite{neurotoxin} proposed a projection method, Neurotoxin, for any backdoor attack method to improve the longevity of the compromise to a model. The attacker's updates are projected onto dimensions with small absolute values of the weight vector. The authors claim such weights are updated less frequently by other benign users, resulting in greater longevity of successful attacks. We will demonstrate our method's effectiveness against both of the above attacks~\cite{dba, neurotoxin}.

\paragraph{Defense.} On the other hand, few defense methods have been proposed to defend against backdoor attacks in FL. Prior work~\cite{shejwalkar2022back} claims that norm clipping~\cite{sun2019can} is effective against backdoor attacks in FL but has been broken by the Neurotoxin attack. Two other robust defense methods for FL were proposed in~\cite{trim-mean}. The paper theoretically explores two robust aggregation methods: Median and Trim-mean, which were shown effective in defending against poisoning attacks in FL. Median is a coordinate-wise aggregation rule in which the aggregated weight vector is generated by computing the coordinate-wise median among the weight vectors of selected users. Trim-mean aggregates the weight vectors by computing the coordinate-wise mean using trimmed values, meaning that each dimension's top and bottom $k$ elements will not be used. We propose a method that can be implemented in addition to other aggregation or model filtering methods. In the experiment, we focus on the original FedAvg~\cite{fedavg} aggregation to show the effectiveness of our proposed method without assistance from additional defense techniques. 

%\begin{definition}[Coordinate-wise median] 
%For vectors $x_i \in \mathbb{R}^{d}$ for $i \in \mathopen[N\mathclose]$, the coordinate-wise median g has element k defined as follows $g^{(k)} = median(\{x_i^{(k)} \mid i \in N\})$.
%\end{definition}

%\begin{definition}[Coordinate-wise Trim-mean]
%For $\beta \in \mathopen[ 0, 0.5 \mathclose)$ and vectors $x_i \in \mathbb{R}^{d}$ for $i \in \mathopen[I\mathclose]$, the coordinate-wise trimmed-mean g has element k defined as:
%\begin{align*}
%g^{(k)} = \dfrac{\sum_{j \in J}{x_j^{(k)}}}{(1 - 2\beta)N}
%\end{align*}
%where $J \subseteq I$ is the collection of indices that do not include the top and bottom $\beta$ proportion of the sorted $x_i^{(k)}$ values.
%\end{definition}


%
\section{Method}

This section describes the motivation and framework for our proposed method, Trusted Aggregation (TAG), which effectively defends against state-of-the-art backdoor attacks. The current defense aggregation methods~\citep{fedavg,trim-mean} are insufficient for preventing attacks of even mild strength. In addition to better model security, our method can improve accuracy for the original classification task compared to the current robust aggregation methods.

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{make_article/make_visuals/visuals/ext_motivation.png}
    \caption{\footnotesize Final hidden layer output distributions (kernel density estimation based) for a {\color{red}\bf backdoor} model (red) and a {\bf clean} model (black). There is an obvious difference between the distributions of the backdoor and clean models for the target label class.}
    \label{fig: motivation}
\end{figure}

\paragraph{Motivation.} We find that the output layer distributions of models returned by malicious users are very different from that of benign users. Figure~\ref{fig: motivation} shows the output distributions of a backdoor model and a clean model on clean input data.  Each neuron in the output layer corresponds to one class, and the backdoor model has a learned association between the backdoor trigger and the target class. We observe that the learned associated comes with a distributional change in the output distribution for the target class. Therefore it implies that with a guaranteed clean model, we should be able to identify whether another candidate model has a backdoor attack by comparing their output distributions on some clean data.
Note that we can observe a discernible difference between malicious and benign user distributions for this target label class. Therefore, we can leverage this difference to detect backdoor attacks.

\paragraph{Detection Framework.}
We assume that there exists one user who we can be confident is trustworthy to place in charge of gate-keeping the global model for updates. The detection method leverages the trusted user to evaluate incoming model weights and determine whether each contribution is allowed to participate in the global model update procedure. The assumption is reasonable as, in reality, the center server will also collect some data to help with the training process, not just blindly relying on the local data from users.

The main idea is to detect user models with an unusually distributed output layer with information from a single trusted user. Moving forward, we will refer to this single trusted user as the validation user. In each communication round, this validation user completes the following steps to generate a threshold for malicious user detection, see Algorithm~\ref{alg:t-agg}

\begin{algorithm}[H]
\caption{Trusted Aggregation \\ 
Notation: Let ${\bm S}$ represent the random subset of users that will submit locally trained models $U_j$ to update the global model $G$, $U_T$ to denote the model from the trusted user, $\mX$ to denote the local data of the trusted user, $\mathcal{D}$ to represent the distributional difference function, and $\theta \in [1, 2]$ for the method's scaling coefficient.
}
\label{alg:t-agg}
\begin{algorithmic}[1]

    \Procedure{Trusted Aggregation}{$\mX, G, U_T, \{U_j\}_{j \in {\bm S}} \mid \theta$}
        %\State Get the local data $\mX$ from the trusted user
        \State Generated outputs: $\vo_G = G(\mX)$, $\vo_{T} = U_T(\mX)$, and $\vo_j = U_j(\mX)$, $\forall j \in \mS$
        \For{each class $c\in [1,...,m]$}
            \State Compute the distributional distances between each user and the global model
            \State \quad $v^{(c)}_T = \mathcal{D}(\vo^{(c)}_G, \vo^{(c)}_T)$ and $v^{(c)}_{j} = \mathcal{D}(\vo^{(c)}_G, \vo^{(c)}_{j})$, $\forall j \in \mS$
            \Comment{$\vo^{(c)}$: output for class $c$}
        \EndFor
        \State The above procedure produces: $\vv_T\in\sR^m, \vv_j\in\sR^m$
        \Comment{$m$: total number of classes}
        \State Compute threshold: $\tau = \theta \times\max ( \vv_T )$
        \Comment{$\max$: maximum element of the vector}
        \State  $\tilde{\tau} \gets \Call{Global-Min Mean Smoothing}{\tau}$  \Comment{Algorithm \ref{alg:smoothing}}
        \State Select users: $\mS_r = \{j \in \mS | \max(\vv_{j}) < \tilde{\tau} \}$
        \Comment{maximum element $<$ threshold}
        \State \Return FedAvg$(\{U_j\}_{j \in {\bm S}_r})$ 
    \EndProcedure
\end{algorithmic}
\end{algorithm}

In general, Algorithm~\ref{alg:t-agg} determines which users will be used for the global updates based on a threshold. During each round of training, we compute and store a forward pass output ($\vo_G$) of the global model on the validation user's local data. Then, local training is performed, and forward pass outputs ($\vo_j, \vo_T$) on the validation user's local data with the selected users' models and the model of the validation user are stored. For each class, we compute the class-conditional distributional distance ($v^{(c)}_j, v^{(c)}_T$) between the global model output ($\vo^{(c)}_G$) and the user output ($\vo^{(c)}_j$ or $\vo^{(c)}_T$) by applying a distributional difference function on estimated CDFs based on $\vo^{(c)}_G$, $\vo^{(c)}_j$ and $\vo^{(c)}_T$. Here, $\vo^{(c)}$ represents the outputs based on the trusted user's local data with the label $c$. In our experiment, the Kolmogorov-Smirnov (KS) function is used to compute the distributional difference, but other distance functions can also be applied. Suppose there are $m$ classes in total; the process will result in a distance vector ($\vv_j, \vv_T\in\sR^m$) for each user, including the validation user. The distance vectors will then determine which users can be selected for the update.

\paragraph{Threshold Construction.} In this part, we discuss how to decide the threshold ($\tau$) and how to use it to select users.  We quantify the threshold as the largest possible change a non-malicious user could contribute. Users with distance values exceeding the threshold will be excluded. Assume that the class-conditional distances ($v^{(c)}$) are Uniform on $[0, b_c]$ for each class $c$, where $b_c$ is the maximum possible change to the output layer of class $c$ through local training by a non-malicious user.  Therefore, the threshold can be generated by estimating the maximum of $b_c$ for any class.  Let $m$ represent the total number of classes, equation~\ref{eq: dep} gives simple bounds for our quantity of interest under arbitrary dependence. Therefore, we parameterize our method by $\theta \in [0, 1]$, see Algorithm~\ref{alg:t-agg}. We recommend choosing $\theta$ based on the setting's prevalence of backdoor attacks and the cost of a successful attack to interested parties. For example, twice the maximum of the class-conditional distance ($2V$) is a liberal estimator for the upper bound of $b_c, \forall c\in [1,...,m]$. Such scaling would allow more users to update the global model but increase the risk of a successful backdoor attack. Conversely, using $V$ without scaling would help to prevent backdoor attacks, but with the potential loss of denying benign users from updating the global model. The goal is to choose the smallest $\theta$ that allows benign users to sufficiently create a robust shared machine learning model.

If we additionally assume independence between the class conditional distances, we can establish an additional lower bound, equation~\ref{eq: indep}, for our largest possible benign change. Note, equation~\ref{eq: indep} converges asymptotically to the earlier lower bound as the size of the classification problem ($m \rightarrow \infty$) grows. Hence, this additional bound is provided to assist with choosing an appropriate $\theta$ for smaller-way classification problems.

\begin{align}
    & \textrm{Assume } \forall c \in [1,...,m], v^{(c)} \sim \textrm{ Unif}(0, b_c) \nonumber \\
    & \textrm{Define } b = \max_c (b_c), j = \argmax_c (b_c) \textrm{ and } V = \max_c \left( v^{(c)} \right) \nonumber \\
    & \quad v^{(j)} \leq V \leq b_j \Longrightarrow E \left[ v^{(j)} \right] = \dfrac{b_j}{2} \leq E \left[ V \right] \leq b_j  \Longrightarrow E \left[ V \right] \leq b_j \leq E \left[ 2 V \right] \label{eq: dep} \\
    & \textrm{Assuming instead } \forall c \in [1,...,m], v^{(c)} \overset{\textrm{ind}}{\sim} \textrm{ Unif}(0, b_j) \nonumber \\
    & \quad \textrm{Define } W = \max_c \left( w^{(c)} \right) \textrm{ where } w^{(c)} \overset{\textrm{iid}}{\sim} \textrm{ Unif}(0, b_j) \Longrightarrow \dfrac{W}{b_j} \sim \textrm{Beta}(m, 1) \Longrightarrow E(W) = \dfrac{m}{m + 1} \nonumber \\
    & \quad F_V(t) = P(V \leq t) = \prod_{i = 1}^m P_{v^{(c)}}(t) \geq \prod_{i = 1}^m P_{w^{(c)}}(t) = P(W \leq t) = F_W(t) \nonumber \\
    & \quad \Longrightarrow E(V) = \int_0^\infty (1 - F_V(t)) \,dt \leq \int_0^\infty (1 - F_W(t)) \,dt = E(W) \nonumber \\
    & \quad \Longrightarrow E \left[ \dfrac{V}{b_j} \right] \leq E \left[ \dfrac{W}{b_j} \right] = \dfrac{m}{m + 1} \Longrightarrow E \left[ \left( \dfrac{m + 1}{m} \right) V \right] \leq b_j \label{eq: indep} \\
    \nonumber 
\end{align}


Since the validation user is non-malicious, their distance vector serves as a good representation for other non-malicious users. Therefore, we estimate the threshold $\tau$ by setting $\tau=2\times\max(\vv_T)$, where $\vv_T\in\sR^m$ is the distance vector of the validation user and $\max(\cdot)$ means getting the maximum value of the vector $\vv_T$. Then, the maximum distance value ($\max(\vv_j)$) of each selected user will be compared with the threshold ($\tau$) to determine the final list of users who can participate in the update. A user with a maximum distance smaller than the threshold is considered a benign user, while a user with a maximum distance larger than or equal to the threshold will be removed. However, this naive threshold is very unstable, and a lucky malicious user can get past it in some rounds due to the instability. Therefore, we make an additional modification, {\bf global-min mean smoothing}, to this basic threshold to address the concern.

\paragraph{Global-Min Mean Smoothing.} 
A straightforward way to stabilize the threshold value is smoothing methods. However, in the early communication rounds, the naive threshold value rapidly decreases as the model starts making connections between inputs and output classes. Therefore, applying a smoothing method early will result in a relatively high threshold, which may let attackers bypass it. When the naive threshold ($\tau$) decreases rapidly, we do not wish to use any previous communication rounds for the smoothing. 

\pagedepth\maxdimen
\begin{wrapfigure}{r}{.4\textwidth}
    \includegraphics[width=.4\textwidth]{make_article/make_visuals/visuals/smoothing--d_rounds30.png}
    \caption{\footnotesize Comparison of the global min-mean smoothing with the base (naive) threshold and various smoothing methods. 
    %The global min-mean is smooth and captures the early round behavior of the base threshold.
    }
    \label{fig: smoothing}
\end{wrapfigure}

Therefore, we propose to use the lowest observed value (Global Min) of $\tau$ as the starting point of smoothing. Let $\tau_t$ represent the naive estimation of the threshold in round $t$, the smoothed threshold $\tilde{\tau}$ at round $n$ is given by
\begin{align*}
    \tilde{\tau} = \frac{1}{n-t_s+1}\sum_{t=t_s}^n \tau_t,
\end{align*}
where $t_s$ is the round that when the global min is observed. Details of the global-min mean smoothing is described in Algorithm~\ref{alg: smoothing}. As $\tau_t$ shrinks, we observe new global minimums, and the start of the threshold smoothing is reset.  In addition, when our estimate stabilizes, previous values are leveraged to smooth the threshold, which keeps lucky malicious users from getting past a volatile threshold. Figure~\ref{fig: smoothing} compares our global min-mean smoothing with the naive threshold and various smoothing techniques. The global min-mean smoothing best captures the naive threshold's early behavior while providing remarkable stability improvements. Additionally, when our threshold encounters a new global minimum, it provides a conservative estimate to prevent malicious users while re-learning cutoff behavior over the next few rounds.

\begin{algorithm}[H]
\caption{Global-Min Mean Smoothing \\ 
Notation: Let $(\tau_1, \cdots, \tau_{n - 1}, \tau_n)$ denote the sequence of values that we wish to smooth.
}
\label{alg: smoothing}
\begin{algorithmic}[1]

    \Procedure{Global-Min Mean Smoothing}{$\tau_1, \cdots, \tau_{n - 1}, \tau_n$}
        \State Record the location of global minimum: $i = \argmin\limits_{t \in [1,...,n] } \tau_t$
        \State Subset to a sequence starting with the global min: $\{\tau_t\}_{t=i}^n = \{\tau_i, \cdots, \tau_n \}$
        \State \Return average of sequence subset, $\overline{\{\tau_t\}_{t=i}^n}$
    \EndProcedure
\end{algorithmic}
\end{algorithm}

\section{EXPERIMENTS}

\subsection{Setting}

\paragraph{Federated Learning.} We start by giving further specifications regarding the federated learning environment. Our interest is training a global model over $M$ communication rounds with $N$ users. Each iteration randomly selects $K$ users, using a specified proportion of the total users, to participate in the model update. After local training, the next global model is the average returned model weights by the FedAvg procedure. We focus  our experiments on the ResNet18 model architecture; a standard object recognition classifier initially proposed in~\cite{resnet}. We assume that all users, including malicious, have complete control over all aspects of local training, such as learning rate, the number of epochs, and the model weights they return. For simplicity, we select two main sets of training hyper-parameters for benign and malicious users. The malicious users will poison a given proportion of their local data by adding their backdoor trigger to the input and changing the training label to the target class. They intend for the model to associate the trigger with the target class and hence have the future global model identify any input with the trigger as belonging to the target class. 

\paragraph{Attack and Baseline.} To show the effectiveness of our method, we choose a setting in which the backdoor attack is strong. We force all malicious users to be included in the subset of selected users to update the global model each round after the start of the backdoor attack. Note that the selection of random users is a defense against malicious users by making it difficult for them to update the global model repeatedly. Additionally, we do not allow the validation user, a guaranteed benign user, to participate in any global model updates. We make these decisions to show the ability of our threshold to prevent even strong backdoor attacks against the global model. 
For our experiment, we test the proposed method and two other robust aggregation methods, Median and Trim-mean~\citep{trim-mean}, against two state-of-the-art backdoor attacks in FL: Neurotoxin~\citep{neurotoxin} and Distributed Backdoor Attacks (DBA)~\citep{dba}. To further evaluate the effectiveness of the aggregation methods, we also vary the proportion of malicious attackers ($10\%, 40\%$) in selected users to test the defense methods under different attack strength levels.

\paragraph{Data.} The experiments are done on three different data sets: CIFAR10~\citep{krizhevsky2009learning}, STL10~\citep{coates2011analysis} and CIFAR100~\citep{krizhevsky2009learning}. In each experiment, we construct user data sets by random sampling from the training data. For global model evaluation, we split the test set into two parts. We add the backdoor trigger to images in the second half and remove any target class observations. We measure model performance with classification accuracy using the first half as {\color{blue}classification accuracy}, and the proportion of the poisoned half predicted as the target class, known as {\color{red}attack success rate}, to measure the extent that the backdoor attack has compromised the model. 
For a defense method, a good performance consists of a low attack success rate and high classification accuracy. In other words, both attacks are unsuccessful when the defense method is used, and the defense does not negatively influence the classification performance. 
%See more details of hyper-parameters in Appendix.


\subsection{Main Results}

We begin by considering a setting where 10\% of the selected users are malicious each communication round. Figure~\ref{fig: accuracy--n_malicious1} shows the performance of the three robust aggregation methods against backdoor attacks with and without the Neurotoxin projection on three data sets regarding classification accuracy and attack success rate. Our proposed method (TAG) nullifies the backdoor attack in each case without decreasing the classification accuracy of the original task in general. We see a slight improvement in classification accuracy for the CIFAR10 data set and a mild decrease in performance on STL10 under the Neurotoxin attack. However, the other two robust aggregation methods, coordinate-wise Median and Trim-mean, fail to prevent backdoor attacks with or without Neurotoxin. We conclude that our method is a clear improvement to the existing robust aggregation methods for federated learning. 

\begin{figure}[htp]
\centering
  \begin{subfigure}{\textwidth}
  \centering
    \includegraphics[height=4cm, width=\textwidth]{make_article/make_visuals/visuals/accuracy--n_malicious1--dba0--beta0.2--d_scale.png}
    \caption{\footnotesize Performance Under Backdoor Attack With 10\% Malicious Users.}
  \end{subfigure}%

  \begin{subfigure}{\textwidth}
  \centering
    \includegraphics[height=4cm, width=\textwidth]{make_article/make_visuals/visuals/accuracy--n_malicious1--dba0--beta0.2--d_scale--neuro_p0.1.png}
    \caption{\footnotesize Performance Under Neurotoxin Backdoor Attack With 10\% Malicious Users.}
  \end{subfigure}%
\caption{\footnotesize Model performance under standard and Neurotoxin backdoor attacks with 10\% malicious users. The proposed method TAG performs well in defending against backdoor attacks as the attack success rates are low. Meanwhile, it does not generally affect the model's classification performance on clean data, except for a slight decrease for the Neurotoxin attack on STL10. However, the other two aggregations methods do not work well against any of the backdoor attacks.} 
\label{fig: accuracy--n_malicious1}
\end{figure}

We show that TAG can handle even stronger attack settings against state-of-the-art attacks in the following part. We consider testing the robust aggregation methods against DBA and Neurotoxin attacks with 40\% malicious users in the selected set.  These attacks are catastrophically successful against the current robust aggregation methods, see Figure~\ref{fig: accuracy--n_malicious4}, having a nearly perfect attack success rate after round 50 on all our data sets. However, our method, TAG, overcomes the backdoor extent of the initial rounds to prevent the attack against all data sets. 

\begin{figure}[htp]
\centering
  \begin{subfigure}{\textwidth}
  \centering
    \includegraphics[height=4cm, width=\textwidth]{make_article/make_visuals/visuals/accuracy--n_malicious4--dba1--beta0.2--d_scale.png}
    \caption{\footnotesize Performance Under DBA Backdoor Attack With 40\% Malicious Users.}
  \end{subfigure}%

  \begin{subfigure}{\textwidth}
  \centering
    \includegraphics[height=4cm, width=\textwidth]{make_article/make_visuals/visuals/accuracy--n_malicious4--dba1--beta0.2--d_scale--neuro_p0.1.png}
    \caption{\footnotesize Performance Under DBA And Neurotoxin Backdoor Attacks With 40\% Malicious Users.}
  \end{subfigure}%
\caption{\footnotesize Model performance under DBA and Neurotoxin backdoor attacks with 40\% malicious users. The proposed method TAG performs well in defending against the backdoor attacks for all data sets. Again, the other two aggregation methods do not work well in preventing any backdoor attack under the stronger attack setting.}
\label{fig: accuracy--n_malicious4}
\end{figure}


%
\subsection{Data Distribution Study}

In this section, we show the applicability of our method for imbalanced users. Recall that local data sets are not required to be independent and identically distributed for federated learning. For our experiments, the user data sets are obtained by random sampling from the training data. In this experiment, we use the m-dimensional (number of total classes) Dirichlet distribution to determine the proportion for each class to be randomly sampled. We parameterize this distribution by $\alpha \mathbf{1}_m$ where $\alpha$ is a scalar such that large values correspond to balanced data and smaller values lead to imbalanced user data, and $\mathbf{1}_m$ is an m-dimensional vector of ones.

\begin{figure}[htp]
    \centering
    \includegraphics[height=4cm, width=.5\textwidth]{make_article/make_visuals/visuals/accuracy--cifar_10--n_malicious2--dba1--beta0.2--alpha1.png}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}


%
\subsection{Without Global-Min Mean Smoothing}

% \begin{figure}[htp]
%     \centering
%     \includegraphics[height=4cm, width=.5\textwidth]{}
%     \caption{Caption}
%     \label{fig:my_label}
% \end{figure}


%
\subsection{Validation User Data Proportion Study}


% 
\section{Conclusion}

We believe our proposed method, Trusted Aggregation (TAG), is an essential advancement toward model security for the federated learning framework. While current robust aggregation methods fail to prevent mild backdoor attacks, TAG holds up against state-of-the-art attacks in unreasonably strong settings. Furthermore, TAG can act as a layer of model filtering in addition to current and future modifications to the choice of aggregation step.

\newpage
{  \small 
\bibliographystyle{iclr2023_conference}
\bibliography{iclr2023_conference}
}


%
\pagebreak
\appendix

%
\section{Class Frequency Scaling}

\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
    \includegraphics[width=0.4\textwidth]{make_article/make_visuals/visuals/scaling.png}
    \caption{Class Frequency Scaling}
    \label{fig: scaling}
\end{wrapfigure}

\paragraph{Class Frequency Scaling.} In some cases, having a trusted user with a balanced distribution of class labels may not be possible. The ability to construct a cutoff from unbalanced data helps when data cannot be collected and avoids data privacy or security issues associated with sharing one's local class labels, or distribution thereof, with other users. From initial experimentation, we saw that either over or under-represented classes result in larger KS values for that class than benign users with a balanced distribution of class labels. Hence, we propose a simple quadratic-based scaling, see Figure \ref{fig: scaling} to scale down both over and under-represented class labels. We fit a polynomial that does not scale a balanced class and scales a class with either no representation or greater than twice the balanced frequency to zero.

\begin{algorithm}[H]
\caption{ (Class Frequency Scaling) \\ 
Notation: Let $x$ denote the vector we wish to scale and $y$ be the vector of class labels where $D$ represents the total number of unique classes for the classification problem. 
}
\label{alg:scaling}
\begin{algorithmic}[1]
    \Procedure{Class Frequency Scaling}{$\{x_c \mid c \in \mathopen[ D \mathclose] \} \mid y$}
    \State define balanced proportion for classes, $B = 1/D$
        \For{each class $c \in \mathopen[ D \mathclose]$}
            \If{class $c$ is over-represented, $x_c > 2B$}
                \State $s_c = 0$
            \Else
                \State $s_c = \dfrac{-1}{B^2}(x_c)(x_c - 2B)$
            \EndIf
        \EndFor
    \State \Return $\{s_c * x_c \mid c \in \mathopen[ D \mathclose] \}$
    \EndProcedure
\end{algorithmic}
\end{algorithm}


\end{document}
