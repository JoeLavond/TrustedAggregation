\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference, times}

% --- Default packages --- 
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{hyperref}
\usepackage{url}

% --- Custom packages --- 
% images
\usepackage{graphicx}
\usepackage{float}
\usepackage{wrapfig}

% psudeocode
\usepackage{algorithm}
\usepackage{algpseudocode}
\def\NoNumber#1{{\def\alglinenumber##1{}\State #1}\addtocounter{ALG@line}{-1}}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}

\newcommand{\yli}[1]{{\color{cyan}#1}}

\title{
    Trusted Aggregation (TAG): Model Filtering Backdoor Defense In Federated Learning
}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{%
    Joseph Lavond \& Yao Li  \\
    Department of Statistics and Operations Research \\
    University of North Carolina at Chapel Hill \\
    Chapel Hill, NC 27514, USA \\
    \texttt{\{jlavond, yaoli\}@unc.email.edu} \\
    %
    \And
    Minhao Cheng \\
    Computer Science and Engineering \\
    Hong Kong University of Science and Technology \\
    Clear Water Bay, Hong Kong \\
    \texttt{minhaocheng@ust.hk}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
    Federated Learning (FL) is a framework for constructing robust machine learning models from multiple local data sets without aggregation. A robust shared model is learned through an interactive process that combines locally learned model gradients or weights. However, the lack of data transparency naturally raises concerns about model security. Several state-of-the-ark backdoor attacks have recently been created to take advantage of the FL setting to get the shared model to learn a specific behavior for inputs given a particular manipulation. Motivated by differences in the output layer distribution between models trained with and without the presence of backdoor attacks, we propose the first meaningful defense specific to FL that can prevent stronger backdoor attacks and improve accuracy for the original classification task compared to the current aggregation or model filtering methods.
\end{abstract}



% ----- Content -----
\section{Introduction}

Federated learning (FL) is a potential solution to constructing a machine learning model from several local data sources that cannot be exchanged or aggregated. As mentioned in \cite{fed-learn}, these restrictions are essential in areas where data privacy or security is critical, including but not limited to healthcare and defense. Additionally, FL is valuable for companies wishing to shift computing workloads to local devices. Furthermore, these local data sets are not required to be independent and identically distributed. Hence, a shared robust global model is desirable and, in many cases, cannot be produced without some form of collaborative learning. Under the FL setting, local entities submit their locally learned model gradients and weights to be intelligently combined by some centralized entity to create a shared and robust machine learning model.


Concerns have arisen that the lack of control or knowledge regarding the local training procedure could allow a user, with malicious intent, to create an update that compromises the global model for all future users. An example of such harm is a backdoor attack, where training attempts to get a model to associate a given manipulation of the input data, known as a trigger, with a particular outcome. Recent work, such as by \cite{stamp-invisible}., creates backdoor triggers that are not detectable in the data by either human or computer vision when included within the data. However, in FL, only the resulting model gradients or weights are communicated back, so there is potentially no need to hide the trigger in the data. Furthermore, without access to leverage user input data, there is less information available to help detect and prevent such malicious intent. Thus backdoor attacks may be easier to perform and harder to detect in FL.

Our contribution to federated learning is establishing defense criteria effective against multiple attackers and state-of-the-ark attacks. We demonstrate our method's ability on several data sets to prevent backdoor attacks, regardless of whether the attacks occur even in every update to the shared model or starting at the beginning of the federated learning process. Furthermore, our threshold still allows many regular users to update the global model, resulting in no decrease and several increases in the accuracy of the original classification task.

%
\section{Related Work}

\paragraph{Federated Learning.} At a high level, federated learning is an iterative procedure involving rounds of model improvement until it meets some criteria. These rounds send the global model to users and select a subset of users to update the global model. Then those chosen users train their local copy of the model, and their resulting models are communicated back and aggregated to create a new global model. Typically, the final local model's gradients or weights are transmitted back to ensure data privacy.

\paragraph{Backdoor Attack \& Defense.} Recently, several backdoor attacks have been proposed to take advantage of the federated learning setting. First, \cite{dba} shows that the multiple-user nature of federated learning is exploitable to make more potent and lasting backdoor attacks. By distributing the backdoor trigger across a few malicious users, they could make the global model exhibit the desired behavior at higher rates and for many iterations after the attack had concluded. We will show our threshold's effectiveness in even more potent attack settings than in their original paper. 

Secondly, \cite{neurotoxin} proposed a projection for any attack method to improve the longevity of the compromise to a model. The attacker's updates are projected onto some proportion of the smallest in absolute value model weights. The authors claim such weights are updated less frequently by other users, resulting in greater longevity of successful attacks. We will demonstrate our model's effectiveness against both of the above attacks.

Next, \cite{trim-mean} proposed the current state-of-the-ark defense method for federated learning. Their paper theoretically explores two robust aggregation methods of user-returned model weights. In our experiments, we will compare our proposed method to both their coordinate-wise median and trimmed-mean procedures.

\begin{definition}[Coordinate-wise median] 
For vectors $x_i \in \mathbb{R}^{d}$ for $i \in \mathopen[N\mathclose]$, the coordinate-wise median g has element k defined as follows $g^{(k)} = median(\{x_i^{(k)} \mid i \in N\})$.
\end{definition}

\begin{definition}[Coordinate-wise trimmed-mean]
For $\beta \in \mathopen[ 0, 0.5 \mathclose)$ and vectors $x_i \in \mathbb{R}^{d}$ for $i \in \mathopen[I\mathclose]$, the coordinate-wise trimmed-mean g has element k defined as:
\begin{align*}
g^{(k)} = \dfrac{\sum_{j \in J}{x_j^{(k)}}}{(1 - 2\beta)N}
\end{align*}
where $J \subseteq I$ is the collection of indices that do not include the top and bottom $\beta$ proportion of the sorted $x_i^{(k)}$ values.
\end{definition}

Currently, many defense methods are based on modifications to the aggregation method. We propose a method that can be implemented in addition to other aggregation methods or model filtering. However, we focus on the original FedAvg aggregation procedure, proposed by \cite{fedavg} to show the effectiveness of our proposed method without assistance from any additional modifications. Under FedAvg, the aggregation step consists of setting the next iteration of the global model to the average of user-returned updated models. 


%
\section{Method}

This section describes the motivation and framework for our proposed method, Trusted Aggregation (TAG), which effectively defends against state-of-the-ark backdoor attacks. The current defense aggregation methods are insufficient for preventing attacks of even mild strength. In addition to greater model security, our method can improve accuracy for the original classification task compared to current robust aggregation methods.

\paragraph{Motivation}

We find that the output layer distributions of malicious users are very different from that of benign users. Note that there is a discernible difference between malicious and benign user distributions for the target label class. Therefore, we can leverage this difference to detect backdoor attacks. Figure \ref{fig: motivation} shows a model with different estimated distributions for the target class depending on whether or not that model has been backdoor attacked. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{make_article/make_visuals/visuals/ext_motivation.png}
    \caption{Kernel Density Estimation from Predicted Scores}
    \label{fig: motivation}
\end{figure}

\paragraph{Detection Framework}

We minimally assume that there exists one user who we can be confident is trustworthy to place in charge of gate-keeping the global model for updates. The detection method leverages the trusted user to evaluate incoming model weights and determine whether each contribution is allowed to participate in the global model update procedure. The main idea is to detect user models with an unusually distributed output layer with information from a single trusted user. Moving forward, we will refer to this single trusted user as the validation user. In each communication round, this validation user completes the following additional steps to generate a threshold for malicious user detection, see Algorithm \ref{alg:t-agg}

\begin{algorithm}[H]
\caption{ (Trusted Aggregation) \\ 
Notation: Let $S$ represent the random subset of users that will submit locally trained models $U$ to update the global model $G$, $T$ to denote the trusted user, and $D$ to identify some distributional difference function.
}
\label{alg:t-agg}
\begin{algorithmic}[1]

    \Procedure{Trusted Aggregation}{G, U_T, $\{U_j \mid j \in S\}$}
        \For{each class with trusted user's data $(X, y)$}
            \State predict scores: $o_G = G(X)$, $o_{T} = U_T(X)$, and $o_j = U_j(X)$, $\forall j \in S$
            \State calculate empirical CDFs for each predicted score above 
            \State \quad and compute the distributional distances between each user and global models 
            \State \quad $v_T = D(o_G, o_T)$ and $v_{j} = D(o_G, o_{j})$, $\forall j \in S$
            %\State $v_T \gets \Call{Class Frequency Scaling}{v_T}$  \Comment{(Algorithm \ref{alg:scaling})}
        \EndFor
        \State compute round threshold $c_r = 2 * \max ( \{v_T\} )$
        \State  $c_r \gets \Call{Global-Min Mean Smoothing}{c_r}$  \Comment{(Algorithm \ref{alg:smoothing})}
        \State define $S_r = \{j \in S | \max (\{v_{j}\}) < c_r \} \subseteq S$
        \State \Return FedAvg$(\{U_j \mid j \in S_T \})$ 
    \EndProcedure
\end{algorithmic}
\end{algorithm}


%
\begin{wrapfigure}{r}{.5\textwidth}
\begin{align}
    & \textrm{Suppose } \forall c \in \mathopen[ D \mathclose], v_c \sim \textrm{ Uniform}(0, b_c) \nonumber \\
    & \textrm{Let } j = \argmax_c \left( b_c \right) \textrm{ such that } b_j = \max_c \left( b_c \right) \nonumber \\
    & \quad \max_c \left( v_c \right) \geq v_j \nonumber \\
    & \quad E \left[ \max_c \left( v_c \right) \right] \geq E \left[ v_j \right] = \dfrac{b_j}{2} \nonumber \\
    & \quad E \left[ 2 * \max_c \left( v_c \right) \right] \geq b_j \label{eq:geq_bound} 
\end{align}
\end{wrapfigure}

\paragraph{Threshold Construction.} Recall that we intend to determine the most considerable possible change that a non-malicious user could contribute and exclude all users from participating in the update of the global model that exceeds such a threshold. Hence, we want to estimate the most prominent possible distributional distance for any class for use as a cutoff. We operate under the assumption that the class distances are Uniform on $\mathopen[0, b_c\mathclose]$ for each class c, where $b_c$ is the maximum possible change to the output layer of class c through local training by a non-malicious user. Therefore, we aim to estimate and use a representation for the maximum of $b_c$, the maximal benign change to any class, as a cutoff for returning user models. Letting $D$ represent the total number of classes, Equation \ref{eq:geq_bound} shows, under our distributional assumptions, twice the maximum of the class conditional differences is, on average, a practical upper bound for accepting benign submissions.

Additionally, we make an additional modification, global-min mean smoothing, to this basic threshold to address the following concerns:
\begin{enumerate}
    \item In early communication rounds, the cutoff rapidly decreases as the model starts making connections between inputs and output classes. Malicious users have ample opportunity to impact the model at the beginning of Federated Learning.
    \item There is extreme instability round-to-round in our threshold. A lucky malicious user can get past a large cutoff for that round due to instability.
    %\item The class distributional differences are more considerable and variable for users with imbalanced classes. Hence, the threshold values depend heavily on the distribution of class labels for the validated user.
\end{enumerate}

%Hence, we refocus our efforts on estimating an upper bound for the most considerable change a non-malicious user with balanced data can make to the output layer of the global model. Our final threshold includes global min-mean smoothing and class frequency scaling to address the above concerns.

%
\paragraph{Global-Min Mean Smoothing. } We needed to develop a stable cutoff that is a meaningful estimate of the maximum distance a benign user can submit for a given round. We use averaging as a valuable solution for reducing variability without impacting bias. Under the assumption that certain consecutive rounds have the same values of $b_c$, we can use the average threshold across several rounds to improve our estimate's stability. Note that we can view this average as smoothing across previous values with equal weight. Although smoothing aids stability, smoothing methods tend to exacerbate the problem resulting from $b_c$ tending to decrease rapidly over the first few communication rounds. When $b_c$ decreases rapidly, we do not wish to use any previous communication rounds for our threshold as this would result in a weaker cutoff for attacks to overcome. 

\begin{wrapfigure}{r}{.45\textwidth}
    \includegraphics[width=.45\textwidth]{make_article/make_visuals/visuals/smoothing--d_rounds30.png}
    \caption{Comparison of Various Smoothing Methods}
    \label{fig: smoothing}
\end{wrapfigure}

Our solution is to use as our cutoff the running mean since the lowest observed value (Global-Min Mean) of the maximum class distance for the validated user. As a result, our solution quickly adapts to decreasing values of $max_c (b_c)$ over communication rounds. As $max_c (b_c)$ shrinks, we are likely to observe new global minimums, and the start of the threshold is reset. In addition, when our estimate stabilizes, the averaging smooths our cutoff, which keeps lucky malicious users from getting past a volatile threshold. 

We provide Figure \ref{fig: smoothing} to compare our global min-mean smoothing with the base cutoff and various smoothing techniques. We believe our implementation choice best captures early behavior while providing remarkable improvements to stability. Additionally, when our threshold encounters a new global minimum, it provides a conservative estimate to prevent malicious users while re-learning cutoff behavior over the next few rounds.

\begin{algorithm}[H]
\caption{ (Global-Min Mean Smoothing) \\ 
Notation: Let $x$ denote the vector of values that we wish to smooth.
}
\label{alg:smoothing}
\begin{algorithmic}[1]

    \Procedure{Global-Min Mean Smoothing}{$x_n \mid x_1, \cdots, x_{n - 1}$}
        \State compute location of global minimum over communication rounds, $i = \argmin_{j: j \in \mathopen[ n \mathclose]} x_j$
        \State subset to sequence history since and including global min, $x_i^n = \{x_i, \cdots, x_n \}$
        \State \Return average of sequence subset, $\overline{x_i^n}$
    \EndProcedure
\end{algorithmic}
\end{algorithm}


\section{EXPERIMENTS}

%
\subsection{Setting}

\paragraph{Federated Learning.} We start by giving further specifications regarding the federated learning environment. Our interest is training a global model over M communication rounds with N users. Each iteration randomly selects K users, using a specified proportion of the total users, to participate in the model update. After local training, the next global model is the average returned model weights by the FedAvg procedure. We focus  our experiments on the ResNet18 model architecture, a standard object recognition classifier initially proposed in \cite{resnet}. 

We assume that all users, including malicious, have complete control over all aspects of local training, such as learning rate, the number of epochs, and the model weights they return. For simplicity, we select two main sets of training hyperparameters for benign and malicious users. The malicious users will poison a given proportion of their local data by adding their backdoor trigger to the input and changing the training label to that of the target class. They intend for the model to associate the trigger with the target class and hence have the future global model identify any input with the trigger as belonging to the target class. 

\paragraph{Attack.} To further show the effectiveness of our method, we choose an unrealistic and unreasonable strong attack setting. We force all malicious users to be included in the subset of users to update the global model each round after the start of the backdoor attack. Note that the selection of random users is a defense against malicious users by making it difficult for them to update the global model repeatedly. Additionally, we do not allow the validation user, a guaranteed benign user, to participate in any global model updates. We make these decisions to show the ability of our threshold to prevent even strong backdoor attacks against the global model. 

For our experiment, we vary the proportion of malicious attackers and compare the effectiveness of our method against the alternative aggregation methods coordinate-wise median and trim-mean. We also highlight the effectiveness of our defense against Neurotoxin and Distributed Backdoor Attacks (DBA).

%\paragraph{Data.} To produce local datasets that do not have to be independent and identically distributed, we sample from the training data set using a Dirichlet distribution with specified parameter alpha. The Dirichlet sample determines the proportion of each class included in that user's dataset. Larger values of alpha produce more balanced class distributions. We separately control alpha for benign, malicious, and validation users. Our experiments use the CIFAR-10, CIFAR-100, and STL-10 data sets.

\paragraph{Data.} Our experiments randomly split the CIFAR-10, CIFAR-100, and STL-10 data sets between our users. For global model evaluation, we split the test set into two parts. We add the backdoor trigger to images in the second half and remove any target class observations. We measure model performance with classification accuracy using the first half as model success, and the proportion of the poisoned half predicted as the target class, known as attack success rate, to measure the extent that the backdoor attack has compromised the model.


%
\subsection{Results}

We begin by considering an initial attack where 10\% of the user subset is malicious each communication round. Figure \ref{fig: accuracy--n_malicious1} shows the model's and attack's performance and the attack for several different data sets. In each case, our proposed method (TAG) nullifies the backdoor attack without decreasing the original task's classification rate. Furthermore, the model reaches a clear improvement in the model's classification accuracy on the CIFAR-10 data set.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{make_article/make_visuals/visuals/accuracy--n_malicious1--dba0--beta0.2.png}
    \caption{Model Performance Under Backdoor Attack With 10\% Malicious}
    \label{fig: accuracy--n_malicious1}
\end{figure}

Regarding the existing aggregation methods, coordinate-wise median and trim-mean, neither prevent even the most straightforward backdoor attack we consider on any of our data sets. We conclude that our method is a clear improvement to the existing defense methods for Federated Learning. 

However, we continue to show that our method handles even stronger attack settings and can prevent backdoor attacks from current state-of-the-art attacks in settings that are well beyond reason. Next, we consider defending against Distributed Backdoor Attacks (DBA), where the backdoor trigger is geometrically distributed between the malicious users. This attack's authors, \cite{dba}, proposed this attack to take advantage of the Federated Learning setting to create more stealthy attacks from each user using only a partial representation of the backdoor trigger. 

Figure \ref{fig: accuracy--dba1--n_malicious2} shows model performance against DBA attack where 20\% of the user subset is malicious each round. These attacks are much stronger than those in the previous figure, Figure \ref{fig: accuracy--n_malicious1}, as the baseline robust aggregation methods, coordinate-wise median and trim-mean, succumb entirely to the backdoor attack and in fewer communication rounds. However, TAG continues to completely prevent the backdoor attack on all our data sets.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{make_article/make_visuals/visuals/accuracy--n_malicious2--dba1--beta0.2.png}
    \caption{Model Performance Under Distributed Backdoor Attack With 20\% Malicious}
    \label{fig: accuracy--dba1--n_malicious2}
\end{figure}

Continuing our increase of attack strength, in the following setting, malicious users now comprise 40\% of the user subset. The malicious users perform a Distributed Backdoor Attack and use the Neurotoxin attack projection in their local updates. Recall the Neurotoxin attack projects model updates onto a given proportion of the smallest in absolute value model weights. The authors \cite{neurotoxin} target weights that may be of lesser importance to benign models. 

These attacks are catastrophically successful against the current robust aggregation methods, see Figure \ref{fig: accuracy--dba1--n_malicious4--neuro1}, having a nearly perfect attack success rate after round 50 on all our data sets. However, our method, TAG, overcomes the backdoor extent of the initial rounds to prevent the attack against both CIFAR data sets. Although our defense method eventually could not withstand the STL-10 backdoor attack, we note that incredibly TAG delayed the attack's success for nearly 90 communication rounds when nearly half of the users were malicious.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{make_article/make_visuals/visuals/accuracy--n_malicious4--dba1--beta0.2--neuro_p0.1.png}
    \caption{Model Performance Under Neurotoxin Distributed Backdoor Attack With 40\% Malicious}
    \label{fig: accuracy--dba1--n_malicious4--neuro1}
\end{figure}

% 
\section{Conclusion}

We believe our proposed method, Trusted Aggregation (TAG), is an essential advancement toward model security for the federated learning (FL) framework. While current robust aggregation methods fail to prevent mild backdoor attacks, TAG holds up against state-of-the-ark attacks in unreasonably strong settings. Furthermore, TAG can act as a layer of model filtering in addition to current and future modifications to the choice of aggregation step.

%
\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}





%
\pagebreak
\appendix

%
\section{Class Frequency Scaling}

\begin{wrapfigure}{r}
    \centering
    \includegraphics[width=.4\textwidth]{make_article/make_visuals/visuals/scaling.png}
    \caption{Class Frequency Scaling}
    \label{fig:scaling}
\end{wrapfigure}

\paragraph{Class Frequency Scaling.} In some cases, having a trusted user with a balanced distribution of class labels may not be possible. The ability to construct a cutoff from unbalanced data helps when data cannot be collected and avoids data privacy or security issues associated with sharing one's local class labels, or distribution thereof, with other users. From initial experimentation, we saw that either over or under-represented classes result in larger KS values for that class than benign users with a balanced distribution of class labels. Hence, we propose a simple quadratic-based scaling, see Figure \ref{fig:scaling} to scale down both over and under-represented class labels. We fit a polynomial that does not scale a balanced class and scales a class with either no representation or greater than twice the balanced frequency to zero.

\begin{algorithm}[H]
\caption{ (Class Frequency Scaling) \\ 
Notation: Let $x$ denote the vector we wish to scale and $y$ be the vector of class labels where $D$ represents the total number of unique classes for the classification problem. 
}
\label{alg:scaling}
\begin{algorithmic}[1]
    \Procedure{Class Frequency Scaling}{$\{x_c \mid c \in \mathopen[ D \mathclose] \} \mid y$}
    \State define balanced proportion for classes, $B = 1/D$
        \For{each class $c \in \mathopen[ D \mathclose]$}
            \If{class $c$ is over-represented, $x_c > 2B$}
                \State $s_c = 0$
            \Else
                \State $s_c = \dfrac{-1}{B^2}(x_c)(x_c - 2B)$
            \EndIf
        \EndFor
    \State \Return $\{s_c * x_c \mid c \in \mathopen[ D \mathclose] \}$
    \EndProcedure
\end{algorithmic}
\end{algorithm}


\end{document}
