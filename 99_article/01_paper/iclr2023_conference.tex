\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference, times}

% --- Default packages --- 
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{hyperref}
\usepackage{url}

% --- Custom packages --- 
% images
\usepackage{graphicx}
\usepackage{float}

% psudeocode
\usepackage{algorithm}
\usepackage{algpseudocode}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}

\newcommand{\indep}{\perp \!\!\! \perp}

\title{Title \\ Subtitle}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{%
    Joseph Lavond \& Yao Li  \\
    Department of Statistics and Operations Research \\
    University of North Carolina at Chapel Hill \\
    Chapel Hill, NC 27514, USA \\
    \texttt{\{jlavond, yaoli\}@unc.email.edu} \\
    %
    \And
    Minhao Cheng \\
    Computer Science and Engineering \\
    Hong Kong University of Science and Technology \\
    Clear Water Bay, Hong Kong \\
    \texttt{minhaocheng@ust.hk}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Abstract text here.
\end{abstract}



% ----- Content -----
\section{Introduction}

Federated learning is a potential solution to constructing a machine learning model from several local data sources that cannot be exchanged or aggregated. These restrictions are essential in areas where data privacy or security is critical such as healthcare and defense. However, federated learning is also valuable to companies that can shift much of the computing workload to local devices instead. Furthermore, the local datasets are not required to be independent and identically distributed. Hence, a shared robust global model is desirable and, in many cases, cannot be produced without some form of collaborative learning.


Concerns have arisen that the lack of control or knowledge regarding the local training procedure could allow a user, with malicious intent, to create an update that compromises the global model for all future users. An example of such harm is a backdoor attack, where training attempt to get a model to associate a given manipulation of the input data, known as a trigger, with a particular outcome. Recent work, such as by \cite{stamp-invisible}., creates backdoor triggers that are not detectable in the data by human or computer vision. However, in federated learning, only the resulting model gradients or weights are communicated, so there is potentially no need to hide the trigger in the data. Furthermore, without access to leverage user input data, there is less information available to help detect and prevent such malicious intent.

Our contribution to federated learning is establishing defense criteria for federated learning, which is effective against multiple attackers, model scaling, and attacks before global model convergence. Furthermore, our threshold still allows many regular users to update the global model, resulting in little to no performance degradation and requiring only a minimal increase in computational resources or time.

%
\section{Related Work}

At a high level, federated learning is an iterative procedure of rounds until it meets some termination criteria. These rounds consist of sending the global model to users and selecting a subset of users to update the global model. Those chosen users train their local copy of the model, and their resulting models are communicated back and aggregated in some manner to create a new global model. Typically, the final local model's gradients or weights are transmitted back to ensure privacy or keep the necessary resources minimal. See Algorithm \ref{alg:fed-learn} for pseudo-code regarding federated learning.


\begin{algorithm}
\caption{}
\label{alg:fed-learn}
\begin{algorithmic}
    \Procedure{Federated Learning}{}
    \State initialize global model $G$
    \For{round $r \in \mathopen[ M \mathclose]$}
        \State send global model to all users, $\forall i \in \mathopen[ N \mathclose], U_i \gets G$
        \State select a random subset of users, $S \subseteq \{1, \cdots, N\}$
        \State the selected users perform local training and return model weights
        \State update global model from returning models, $G \gets f(\{U_j \mid j \in S\})$
    \EndFor
    \State \Return G
    \EndProcedure
\end{algorithmic}
\end{algorithm}

Currently, many defense methods are based on modifications to the aggregation method. We propose a model filtering method that can be implemented in addition to other aggregation methods or model filtering. Since our threshold can be implemented for many aggregation methods, we focus on the original FedAvg procedure, proposed by \cite{fedavg}. Under FedAvg, the aggregation step consists of setting the next iteration of the global model to the average of user-returned updated models in \ref{alg:fed-learn}. 

Next, \cite{dba} shows that the multiple-user nature of federated learning is exploitable to make more potent and lasting backdoor attacks. By geometrically distributing the backdoor trigger across a few malicious users, they could make the global model exhibit the desired behavior at higher rates and for many iterations after the attack had concluded. We will show our threshold's effectiveness in even more potent attack settings than in their original paper.

Finally, \cite{trim-mean} proposed the current state-of-the-ark defense method for federated learning. Their paper theoretically explores two robust aggregation methods of user-returned model weights. In our experiments, we will compare our proposed method to both their coordinate-wise median and trimmed-mean procedures.

\begin{definition}[Coordinate-wise median] 
For vectors $x_i \in \mathbb{R}^{d}$ for $i \in \mathopen[N\mathclose]$, the coordinate-wise median g has element k defined as follows $g^{(k)} = median(\{x_i^{(k)} \mid i \in N\})$.
\end{definition}

\begin{definition}[Coordinate-wise trimmed-mean]
For $\beta \in \mathopen[ 0, 0.5 \mathclose)$ and vectors $x_i \in \mathbb{R}^{d}$ for $i \in \mathopen[I\mathclose]$, the coordinate-wise trimmed-mean g has element k defined as:
\begin{align*}
g^{(k)} = \dfrac{\sum_{j \in J}{x_j^{(k)}}}{(1 - 2\beta)N}
\end{align*}
where $J \subseteq I$ is the collection of indices that do not include the top and bottom $\beta$ proportion of the sorted $x_i^{(k)}$ values.
\end{definition}

%
\section{Method}

We make the additional assumption that there exists one user who we can be confident is trustworthy to place in charge of gate-keeping the global model for updates. That user will evaluate incoming model weights and determine whether each contribution is allowed to participate in the FedAvg procedure. Their decision will be made independently of other users and thus will not violate any data privacy or security concerns.

Our method attempts to detect unusually distributed output layers of the returning user models from this single trusted user. Moving forward, we intend to refer to this single trusted user as the validation user. In each communication round, this user completes additional steps, see Algorithm \ref{alg:m-fed-learn}, to create an estimated upper bound for the most considerable change a non-malicious user can make to the output layer of the global model. Note, until a later section, we omit the details required to calculate our threshold.

\begin{definition}[Empirical Cumulative Distribution Function (CDF)]
For sample $x_i$ for $i \in \mathopen[ N \mathclose]$, the empirical CDF is defined as $\hat{F}_X(t) = \dfrac{\sum_{i=1}^{N} I(x_i \leq t)}{N}$ where I denotes the indicator function.
\end{definition}

\begin{definition}[Kolmogov-Smirnov (KS) Distance]
The KS distributional distance between P and Q (or equivalently their CDFs F and G, respectively) is defined as 
\begin{align*}
    KS(P, Q) = KS(F, G) = \sup_t \mid F(t) - G(t) \mid
\end{align*}
\end{definition}

\begin{algorithm}
\caption{\\The trusted user performs a local update of the global model, $U_T$, simultaneous to the local training of other users, using local data $(X_T, y_T)$. The trusted user's model is then used to determine the largest change to the output distribution possible by a benign user with balanced data.}\label{alg:m-fed-learn}
\begin{algorithmic}
    \Procedure{Trusted Aggregation}{G, U_T, $\{U_j \mid j \in S\}$}
    \For{each class, $c \in \mathopen[ d \mathclose]$}
        \State subset to $X_T^{(c)} = X_T[y_T == c]$
        \State predict scores: $O_G = G(X_T^{(c)})$, $O_{T} = {U_T}(X_T^{(c)})$, and $O_{U}^{(j)} = {U_j}(X_T^{(c)})$, $\forall j \in S$
        \State compute empirical CDFs: $\hat{F}_G^{(c)}$, $\hat{F}_T^{(c)}$, and $\hat{F}_{U_j}^{(c)}$ from predicted scores above
        \State compute KS distances: $v_T^{(c)} = KS(\hat{F}_G^{(c)}, \hat{F}_T^{(c)})$ and $v_{U_j}^{(c)} = KS(\hat{F}_G^{(c)}, \hat{F}_{U_j}^{(c)})$, $\forall j \in S$
    \EndFor
    \State compute cutoff $c_T = g( \{ v_T^{(c)} \mid c \in \mathopen[ d \mathclose] \} )$
    \State let $S_T = \{j \in S | \max_c (v_{U_j}^{(c)}) \leq c_T \} \subseteq S$
    \State \Return aggregate $G \gets f(\{U_j \mid j \in S_T \})$
    \EndProcedure
\end{algorithmic}
\end{algorithm}

Only the validation user's local training, requiring gradient calculations, is an additional step with a high relative computational cost. However, users often train on local compute resources in the Federated Learning setting. Therefore, the validation user will do this step simultaneously with the local training of other users, largely avoiding extra training time. The only other steps not already done during Federated Learning are forward passes, which are less complex and quickly accelerated by modern computing architectures.

We make use of the empirical CDF function to avoid distributional assumptions on the output layers and KS distances for computational efficiency. Using the property that the empirical CDFs are step functions, note that the KS distance between the empirical CDFs only needs to be computed for each of the values of the class scores on the local user's data for both the global and trusted user's updated model, which is easily computed in-parallel.

%
\subsection{Threshold Construction}

Recall that we intend to determine the most considerable possible change that a non-malicious user could contribute and exclude all users from participating in the update of the global model that exceeds such a threshold. Hence, we want to estimate the most prominent possible KS distance for any class for use as a cutoff. We operate under the assumption that the class distances are Uniform on $\mathopen[0, b_c\mathclose]$ for each class c, where $b_c$ is the maximum possible change to the output layer of class c through local training by a non-malicious user. Therefore, we aim to estimate and use a representation for the maximum of $b_c$ ($U$), the maximal benign change to any class, as a cutoff for returning user models. Letting $D$ represent the total number of classes, equation \ref{eq:base_thresh} shows under our distributional assumptions that twice the maximum of the class conditional differences is typically a useful upper bound for benign submissions.

\begin{align}
    & \textrm{Suppose } \forall c \in \mathopen[ D \mathclose], (KS)_c \sim Uniform(0, b_c) \nonumber \\
    & \textrm{Let } j = \argmax_c \left( b_c \right) \textrm{ such that } b_j = \max_c \left( b_c \right) \nonumber \\
    & \quad \max_c \left( (KS)_c \right) \geq (KS)_j \nonumber \\
    & \quad E \left[ \max_c \left( (KS)_c \right) \right] \geq E \left[ (KS)_j \right] = \dfrac{U}{2} \nonumber \\
    & \quad \textrm{So } E \left[ 2 * \max_c \left( (KS)_c \right) \right] \geq U \label{eq:base_thresh} 
\end{align}

Additionally, we make additional modifications to this basic threshold to address the following concerns:
\begin{enumerate}
    \item In early communication rounds, the cutoff rapidly decreases as the model starts making connections between inputs and output classes. Malicious users have a large opportunity to impact the model at the beginning of Federated Learning.
    \item There is extreme instability round-to-round in our threshold. A lucky malicious user can get past a large cutoff for that round due to instability.
    \item The class distributional differences are larger and more variable for users with imbalanced classes. Also, the threshold values depend heavily on the distribution of class labels for the validated user.
\end{enumerate}

Hence, we refocus our efforts more specifically to estimation of an upper bound for the most considerable change a non-malicious user with balanced data can make to the output
layer of the global model. Our final threshold includes a warm-up period, global min-mean smoothing, and entropy-based class scaling to address the above concerns, respectively.

%
\subsection{Warm-up Period}


%
\subsection{Global Min-Mean Smoothing}

We needed to develop a stable cutoff that is a meaningful estimate of the maximum distance a benign user can submit for a given round. By assuming that certain consecutive rounds have the same value of $b_c$, we can use instead the running average of the cutoff across rounds which would have the same expected value with reduced variability. Note that we can view this average as smoothing across all previous values with equal weight. Although smoothing solves our instability problem, smoothing methods exacerbate the second problem, where $b_c$ tends to decrease rapidly over the first few communication rounds. When $b_c$ decreases, we do not wish to use any previous communication rounds for our threshold. Our solution is to use as our cutoff the running mean since the lowest observed value (Global Min-Mean) of the maximum class distance for the validated user. Our solution quickly adapts to decreasing values of $b_c$. As $b_c$ shrinks, we observe new global minimums, and the start of the threshold is reset. In addition, when $b_c$ stabilizes, the averaging smooths our cutoff, keeping lucky malicious users from getting past a volatile threshold.

%
\subsection{Entropy-Based Class Scaling}

In some cases, having a trusted user with a balanced distribution of class labels may not be possible. The ability to construct a cutoff from unbalanced data helps when data cannot be collected and avoids data privacy or security issues associated with sharing one's local class labels with other users. From initial experimentation, we saw that either over or under-represented classes result in larger KS values than benign users with a balanced distribution of class labels. From information theory, Shannon Entropy \footnote{For a discrete random variable $\rvx$ with levels c, entropy is defined $H(x) = - \sum_c P(x_c) \log P(x_c)$} is one such measure of diversity. Note that entropy is maximized when class labels are uniformly distributed. Consider instead each class's contribution towards entropy, $H_c(x) = - P(x_c) \log P(x_c)$, and let D be the total number of classes. We propose scaling the distance between global and updated models for each class for multi-class classification problems by the following factor $S_c$, before computing the maximum and multiplying by two. Note that $S$, the collection of $S_c$, only needs to be computed once, as it depends on the local user's class label distribution and does not depend on the distance values each round.

\begin{align}
    y_c &= \min \left( x_c, \dfrac{1}{e} \right) \\
    z_c &= \min \left( \dfrac{P(y_c) \ln \left( P(y_c) \right)}{\dfrac{1}{D} \ln \left( \dfrac{1}{D} \right)}, 2 \right) \\
    S_c &= 1 - | 1 - z_c |    
\end{align}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{99_article/99_visuals/scaling.png}
    \caption{Entropy Based Scaling ($S_c$) for Several Size Classification Problems}
    \label{fig:scaling}
\end{figure}

See Figure \ref{fig:scaling} for $S_c(x)$ plotted as a function of the class proportions for several values of $D$, the total number of classes. Observe the following  properties:
\begin{enumerate}
    \item Scaling is between 0 and 1, and there is no scaling when a given class is balanced
    \item Under-represented classes are scaled down to zero
    \item For many-class classification problems, we are less trusting of an over-represented class as we have more potential classes for at least one to be approximately balanced.
\end{enumerate}

We believe further research is needed regarding extremely imbalanced edge cases. However, we propose this scaling as a remedy for reasonably imbalanced data, demonstrating its promising effectiveness in our experimentation.

\section{EXPERIMENTS}

\textcolor{red}{Expand results to CIFAR 100 and work on developing comparisons to state-of-the-ark attack and defense.}

%
\subsection{Setting}

We start by giving further specifications regarding the federated learning environment. Our interest is training a global model over M communication rounds with N users. Each iteration randomly selects K users, using a specified proportion of the total users, to participate in the model update. After local training, the next global model is the average returning model weights by the FedAvg procedure. 

To produce local datasets that do not have to be independent and identically distributed, we sample from any training data set using a Dirichlet distribution with specified parameter alpha. The Dirichlet sample determines the proportion of each class included in that user's dataset. Larger values of alpha produce more balanced class distributions. We separately control alpha for benign, malicious, and validation users. Our experiments use the CIFAR-10 dataset, which consists of 60000 32x32 color images in 10 classes, with 6000 images per class. We use the test set of 10000 images, with 500 images per class, for model evaluation. Our experiments use the ResNet18 model architecture, a state-of-the-ark off-the-shelf classifier for object recognition, initially proposed in \cite{resnet}. 

We assume that all users, including malicious, have complete control over all aspects of local training, such as learning rate, the number of epochs, and the model weights they return. The malicious users will poison a given proportion of their local data by adding their backdoor trigger to the input and changing the training label to that of the target class. They intend for the model to associate the trigger with the target class and hence have the future global model identify any input with the trigger as belonging to the target class. For simplicity, we select two main sets of training hyperparameters for benign and malicious users. 

To further show the effectiveness of our method, we choose an unrealistic and unreasonable strong attack setting. We force all malicious users to be included in the subset of users to update the global model each round after the start of the backdoor attack. Note that the selection of random users is a defense against malicious users by making it difficult for them to update the global model repeatedly. Additionally, we do not allow the validation user, a guaranteed benign user, to participate in any global model updates. We make these decisions to show the ability of our threshold to prevent even strong backdoor attacks against the global model. 

For global model evaluation, we split the test set, added the backdoor trigger to half, and removed any target class observations from that half's data. We use classification accuracy from the first half to measure model success, and the proportion of the poisoned half predicted as the target class, known as attack success rate, to measure the backdoor's extent.


%
\subsection{Main Results}
We first demonstrate the performance of our threshold when both the trusted and other users have balanced data in Figure \ref{fig:centralized--alpha10000--alpha_val10000--accuracy--n_malicious1--m_start1}. This figure shows the classification accuracy and the attack success rate of the global model over 250 communication rounds. For this experiment, a single malicious user is included in the subset of users to potentially update the global model every single communication round. The two leftmost figures show, without defense, that the global model can be effectively attacked where the model's classification accuracy does not change. However, the right images show that the use of our defense cutoff prevents backdoor attacks and does not decrease model performance, regardless of whether a backdoor attack is present. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{01_trusted/centralized/alpha10000--alpha_val10000/visuals/accuracy--n_malicious1--m_start1.png}
    \caption{Global Model Performance For Balanced Users And Balanced Trusted User}
    \label{fig:centralized--alpha10000--alpha_val10000--accuracy--n_malicious1--m_start1}
\end{figure}

Figure \ref{fig:centralized--alpha10000--alpha_val10000--threshold--n_malicious1--m_start1--d_rounds50} shows the ability of our threshold to differentiate between benign and malicious user submission. The Threshold vs. Benign figure shows the upper and lower bounds on values considered outliers for each round. We can see our threshold closely follows what we would expect to be the largest possible value a benign user contributes. Additionally, the threshold can clearly distinguish malicious users with at least one larger class output layer distributional difference. 

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\textwidth]{01_trusted/centralized/alpha10000--alpha_val10000/visuals/threshold--n_malicious1--m_start1--d_rounds50.png}
    \caption{Threshold Performance For Balanced Users And Balanced Trusted User}
    \label{fig:centralized--alpha10000--alpha_val10000--threshold--n_malicious1--m_start1--d_rounds50}
\end{figure}

Next, we highlight the ability of our threshold to estimate these benign users' upper bound even when the validation user has imbalanced data. As shown in Figure \ref{fig:centralized--alpha10000--alpha_val1--scaling--d_rounds50.png}, the scaled threshold corrects for the large Kolmogorov Smirnov values of imbalanced classes and gives a reasonable representation for the most considerable value that a benign user with balanced data. The model and threshold performance remain similar to the balanced validation user case; see Appendix \ref{sec.imbal_val} for actual plots. Without scaling, malicious users have an enormous opportunity to backdoor the global model in the early communication rounds. Unfortunately, even with scaling, it still takes several communication rounds to give an accurate cutoff and INSERT COMMENT REGARDING WEAKER AGAINST MULTIPLE ATTACKERS.

\begin{figure}[H]
    \centering
    \includegraphics[width=.5\textwidth]{01_trusted/centralized/alpha10000--alpha_val1/visuals/scaling--n_malicious1--m_start1--d_rounds50.png}
    \caption{Caption}
    \label{fig:centralized--alpha10000--alpha_val1--scaling--d_rounds50.png}
\end{figure}

Note that our threshold is also effective against backdoor attacks when the users have imbalanced data. INSERT COMMENT ABOUT REJECTING MORE BENIGN USERS AND MODEL CONVERGENCE TAKING LONGER. See Appendix \ref{sec.imbal_users} for model performance plots under this setting.

INSERT MULTIPLE ATTACKERS

% 
\section{Conclusion}

%
\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

%
\appendix

%
\section{Threshold Theory}

\begin{align*}
    & \textbf{Case 1: } \forall c \in \mathopen[ D \mathclose], X_c \overset{\mathrm{iid}}{\sim} Uniform(0, U) \\
    \\
    & \dfrac{X_c}{U} \overset{\mathrm{iid}}{\sim} Uniform(0, 1) \\
    & \max_c \left( \dfrac{X_c}{U} \right) \sim Beta(D, 1) \\
    \\
    & E \left[ \max_c \left( \dfrac{X_c}{U} \right) \right] = \dfrac{D}{D + 1} \\
    & E \left[ \dfrac{(D + 1) \max_c \left( X_c \right)}{D} \right] = U\\
    \\
    & \textbf{Case 2: } \forall c \in \mathopen[ D \mathclose], X_c \overset{\indep}{\sim} Uniform(0, b_c) \\
    \\
    & \dfrac{X_c}{b_c} \overset{\mathrm{iid}}{\sim} Uniform(0, 1) \\
    & \max_c \left( \dfrac{X_c}{b_c} \right) \sim Beta(D, 1) \\
    \\
    & \textrm{Let } U = \max_c(b_c) \\
    & \max_c \left( \dfrac{X_c}{U} \right) \leq \max_c \left( \dfrac{X_c}{b_c} \right) \\
    & E \left[ \max_c \left( \dfrac{X_c}{U} \right) \right] \leq E \left[ \max_c \left( \dfrac{X_c}{b_c} \right) \right] = \dfrac{D}{D + 1} \\
    & \dfrac{E \left[ \max_c \left( X_c \right) \right]}{U} \leq \dfrac{D}{D + 1} \\
    & \left( \dfrac{D + 1}{D} \right) E \left[ \max_c \left( X_c \right) \right] \leq U \\
    & E \left[ \dfrac{(D + 1) \max_c \left( X_c \right)}{D} \right] \leq U \\
    \\
    & \textrm{Let } L = \min(b_c) \\
    & \max_c \left( \dfrac{X_c}{U} \right) \leq \max_c \left( \dfrac{X_c}{b_c} \right) \\
    & E \left[ \max_c \left( \dfrac{X_c}{U} \right) \right] \leq E \left[ \max_c \left( \dfrac{X_c}{b_c} \right) \right] = \dfrac{D}{D + 1} \\
    & \dfrac{E \left[ \max_c \left( X_c \right) \right]}{U} \leq \dfrac{D}{D + 1} \\
    & \left( \dfrac{D + 1}{D} \right) E \left[ \max_c \left( X_c \right) \right] \leq U \\
    & E \left[ \dfrac{(D + 1) \max_c \left( X_c \right)}{D} \right] \leq U \\
\end{align*}

%
\section{Main Results Continued}

%
\subsection{Balanced Users, Imbalanced Validation User}
\label{sec.imbal_val}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{01_trusted/centralized/alpha10000--alpha_val1/visuals/accuracy--n_malicious1--m_start1.png}
    \caption{Global Model Performance For Balanced Users And Imbalanced Trusted User}
    \label{fig:centralized--alpha10000--alpha_val1--accuracy--n_malicious1--m_start1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\textwidth]{01_trusted/centralized/alpha10000--alpha_val1/visuals/threshold--n_malicious1--m_start1--d_rounds50.png}
    \caption{Threshold Performance For Balanced Users And Imbalanced Trusted User}
    \label{fig:centralized--alpha10000--alpha_val1--threshold--n_malicious1--m_start1--d_rounds50}
\end{figure}

%
\subsection{Balanced Users, Imbalanced Validation User}
\label{sec.imbal_users}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{01_trusted/centralized/alpha1--alpha_val1/visuals/accuracy--n_malicious1--m_start1.png}
    \caption{Global Model Performance For Imbalanced Users And Balanced Trusted User}
    \label{fig:centralized--alpha1--alpha_val10000--accuracy--n_malicious1--m_start1}
\end{figure} 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{01_trusted/centralized/alpha1--alpha_val1/visuals/accuracy--n_malicious1--m_start1.png}
    \caption{Global Model Performance For Imbalanced Users And Imbalanced Trusted User}
    \label{fig:centralized--alpha1--alpha_val1--accuracy--n_malicious1--m_start1}
\end{figure} 


\end{document}
