\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference, times}

% --- Default packages --- 
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{hyperref}
\usepackage{url}

% --- Custom packages --- 
% images
\usepackage{graphicx}
\usepackage{float}

% psudeocode
\usepackage{algorithm}
\usepackage{algpseudocode}
\def\NoNumber#1{{\def\alglinenumber##1{}\State #1}\addtocounter{ALG@line}{-1}}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}

\newcommand{\indep}{\perp \!\!\! \perp}

\newcommand{\yli}[1]{{\color{cyan}#1}}

\title{
    Trusted Aggregation (TAG): Federated Learning Backdoor Defense
}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{%
    Joseph Lavond \& Yao Li  \\
    Department of Statistics and Operations Research \\
    University of North Carolina at Chapel Hill \\
    Chapel Hill, NC 27514, USA \\
    \texttt{\{jlavond, yaoli\}@unc.email.edu} \\
    %
    \And
    Minhao Cheng \\
    Computer Science and Engineering \\
    Hong Kong University of Science and Technology \\
    Clear Water Bay, Hong Kong \\
    \texttt{minhaocheng@ust.hk}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Abstract text here.
\end{abstract}



% ----- Content -----
\section{Introduction}

Federated learning is a potential solution to constructing a machine learning model from several local data sources that cannot be exchanged or aggregated. These restrictions are essential in areas where data privacy or security is critical such as healthcare and defense. However, federated learning is also valuable to companies that can shift much of the computing workload to local devices instead. Furthermore, the local datasets are not required to be independent and identically distributed. Hence, a shared robust global model is desirable and, in many cases, cannot be produced without some form of collaborative learning. \yli{need citations for this paragraph}


Concerns have arisen that the lack of control or knowledge regarding the local training procedure could allow a user, with malicious intent, to create an update that compromises the global model for all future users. An example of such harm is a backdoor attack, where training attempt to get a model to associate a given manipulation of the input data, known as a trigger, with a particular outcome. Recent work, such as by \cite{stamp-invisible}., creates backdoor triggers that are not detectable in the data by human or computer vision. However, in federated learning, only the resulting model gradients or weights are communicated, so there is potentially no need to hide the trigger in the data. Furthermore, without access to leverage user input data, there is less information available to help detect and prevent such malicious intent. 

Our contribution to federated learning is establishing defense criteria for federated learning, which is effective against multiple attackers, model scaling, and attacks before global model convergence. Furthermore, our threshold still allows many regular users to update the global model, resulting in little to no performance degradation and requiring only a minimal increase in computational resources or time.

%
\section{Related Work}

\paragraph{Federated Learning.} At a high level, federated learning is an iterative procedure of rounds until it meets some termination criteria. These rounds consist of sending the global model to users and selecting a subset of users to update the global model. Those chosen users train their local copy of the model, and their resulting models are communicated back and aggregated in some manner to create a new global model. Typically, the final local model's gradients or weights are transmitted back to ensure privacy or keep the necessary resources minimal. 

\paragraph{Backdoor Attack and Defense.} \cite{dba} shows that the multiple-user nature of federated learning is exploitable to make more potent and lasting backdoor attacks. By geometrically distributing the backdoor trigger across a few malicious users, they could make the global model exhibit the desired behavior at higher rates and for many iterations after the attack had concluded. We will show our threshold's effectiveness in even more potent attack settings than in their original paper.

\cite{trim-mean} proposed the current state-of-the-ark defense method for federated learning. Their paper theoretically explores two robust aggregation methods of user-returned model weights. In our experiments, we will compare our proposed method to both their coordinate-wise median and trimmed-mean procedures.

\begin{definition}[Coordinate-wise median] 
For vectors $x_i \in \mathbb{R}^{d}$ for $i \in \mathopen[N\mathclose]$, the coordinate-wise median g has element k defined as follows $g^{(k)} = median(\{x_i^{(k)} \mid i \in N\})$.
\end{definition}

\begin{definition}[Coordinate-wise trimmed-mean]
For $\beta \in \mathopen[ 0, 0.5 \mathclose)$ and vectors $x_i \in \mathbb{R}^{d}$ for $i \in \mathopen[I\mathclose]$, the coordinate-wise trimmed-mean g has element k defined as:
\begin{align*}
g^{(k)} = \dfrac{\sum_{j \in J}{x_j^{(k)}}}{(1 - 2\beta)N}
\end{align*}
where $J \subseteq I$ is the collection of indices that do not include the top and bottom $\beta$ proportion of the sorted $x_i^{(k)}$ values.
\end{definition}

Currently, many defense methods are based on modifications to the aggregation method. We propose a model filtering method that can be implemented in addition to other aggregation methods or model filtering. Since our threshold can be implemented for many aggregation methods, we focus on the original FedAvg procedure, proposed by \cite{fedavg}. Under FedAvg, the aggregation step consists of setting the next iteration of the global model to the average of user-returned updated models. 


%
\section{Method}

\yli{In this section, we describe our proposed method, NAME OF THE METHOD, which ADVANTAGE OF THE METHOD. The motivation behind the method will be shown in the first part of this section. Then, we introduce the framework of our proposed method.}

\paragraph{Motivation}

We find that the output layer distributions of malicious users are very different from that of benign users. Note that there is a discernible difference between malicious and benign user distributions for the target label class. Therefore, we can leverage this difference to detect backdoor attacks.

\begin{figure}[H]
    \centering
    \includegraphics{99_article/new}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\paragraph{Detection Framework}

We assume that there exists one user who we can be confident is trustworthy to place in charge of gate-keeping the global model for updates. The detection method leverages the trusted user to evaluate incoming model weights and determine whether each contribution is allowed to participate in the global model update procedure. The main idea is to detect user models with an unusually distributed output layer with the information from a single trusted user. Moving forward, we will refer to this single trusted user as the validation user. In each communication round, this validation user completes the following additional steps to generate a threshold for malicious user detection, see Algorithm \ref{alg:t-agg}

\begin{algorithm}[H]
\caption{ (Trusted Aggregation) \\ 
Notation: Let $S$ represent the random subset of users that will submit locally trained models $U$ to update the global model $G$, $T$ to denote the trusted user, and $D$ to identify some distributional difference function.
}
\label{alg:t-agg}
\begin{algorithmic}[1]

    \Procedure{Trusted Aggregation}{G, U_T, $\{U_j \mid j \in S\}$}
        \For{each class with trusted user's data $(X, y)$}
            \State predict scores: $o_G = G(X)$, $o_{T} = U_T(X)$, and $o_j = U_j(X)$, $\forall j \in S$
            \State calculate empirical CDFs for each predicted score above 
            \State \quad and compute the distributional distances between each user and global models 
            \State \quad $v_T = D(o_G, o_T)$ and $v_{j} = D(o_G, o_{j})$, $\forall j \in S$
            \State $v_T \gets \Call{Class Frequency Scaling}{v_T}$  \Comment{(see Algorithm \ref{alg:scaling})}
        \EndFor
        \State compute round threshold $c_r = 2 * \max ( \{v_T\} )$
        \State  $c_r \gets \Call{Global-Min Mean Smoothing}{c_r}$  \Comment{(see Algorithm \ref{alg:smoothing})}
        \State define $S_r = \{j \in S | \max (\{v_{j}\}) < c_r \} \subseteq S$
        \State \Return FedAvg$(\{U_j \mid j \in S_T \})$ 
    \EndProcedure
\end{algorithmic}
\end{algorithm}


%
\paragraph{Threshold Construction.} i Recall that we intend to determine the most considerable possible change that a non-malicious user could contribute and exclude all users from participating in the update of the global model that exceeds such a threshold. Hence, we want to estimate the most prominent possible KS distance for any class for use as a cutoff. We operate under the assumption that the class distances are Uniform on $\mathopen[0, b_c\mathclose]$ for each class c, where $b_c$ is the maximum possible change to the output layer of class c through local training by a non-malicious user. Therefore, we aim to estimate and use a representation for the maximum of $b_c$ ($U$), the maximal benign change to any class, as a cutoff for returning user models. Letting $D$ represent the total number of classes, equation \ref{eq:geq_bound} shows under our distributional assumptions that twice the maximum of the class conditional differences is typically a useful upper bound for benign submissions.

\begin{align}
    & \textrm{Suppose } \forall c \in \mathopen[ D \mathclose], (KS)_c \sim \textrm{ Uniform}(0, b_c) \nonumber \\
    & \textrm{Let } j = \argmax_c \left( b_c \right) \textrm{ such that } b_j = \max_c \left( b_c \right) \nonumber \\
    & \quad \max_c \left( (KS)_c \right) \geq (KS)_j \nonumber \\
    & \quad E \left[ \max_c \left( (KS)_c \right) \right] \geq E \left[ (KS)_j \right] = \dfrac{b_j}{2} \nonumber \\
    & \quad E \left[ 2 * \max_c \left( (KS)_c \right) \right] \geq b_j \label{eq:geq_bound} 
\end{align}

Additionally, we make additional modifications to this basic threshold to address the following concerns:
\begin{enumerate}
    \item In early communication rounds, the cutoff rapidly decreases as the model starts making connections between inputs and output classes. Malicious users have a large opportunity to impact the model at the beginning of Federated Learning.
    \item There is extreme instability round-to-round in our threshold. A lucky malicious user can get past a large cutoff for that round due to instability.
    \item The class distributional differences are larger and more variable for users with imbalanced classes. Also, the threshold values depend heavily on the distribution of class labels for the validated user.
\end{enumerate}

Hence, we refocus our efforts more specifically on estimating an upper bound for the most considerable change a non-malicious user with balanced data can make to the output layer of the global model. Our final threshold includes global min-mean smoothing and class frequency scaling to address the above concerns.

%
\paragraph{Class Frequency Scaling.} In some cases, having a trusted user with a balanced distribution of class labels may not be possible. The ability to construct a cutoff from unbalanced data helps when data cannot be collected and avoids data privacy or security issues associated with sharing one's local class labels with other users. From initial experimentation, we saw that either over or under-represented classes result in larger KS values than benign users with a balanced distribution of class labels. 

\begin{algorithm}[H]
\caption{ (Class Frequency Scaling) \\ 
Notation: Let $x$ denote the vector we wish to scale and $y$ be the vector of class labels where $D$ represents the total number of unique classes for the classification problem. 
}
\label{alg:smoothing}
\begin{algorithmic}[1]
    \Procedure{Class Frequency Scaling}{$\{x_c \mid c \in \mathopen[ D \mathclose] \} \mid y$}
        \For{each class $c \in \mathopen[ D \mathclose]$}
        \EndFor
    \EndProcedure
\end{algorithmic}
\end{algorithm}

%%% WORKING
\begin{minipage}{.4\linewidth}
    \centering
    \includegraphics[width=\textwidth]{99_article/99_visuals/extra_plots/scaling.png}
    \caption{Simple Scaling}
    \label{fig:my_label}
\end{minipage}%
\begin{minipage}{.6\textwidth}
    \begin{align}
        & \textrm{Let } \forall c \in \mathopen[ D \mathclose], p_c = \textrm{ the proportion of labels in each class and } \nonumber \\
        & \textrm{Define } S_T \in \mathbb{R}^D \textrm{ as:} \nonumber \\
        & \quad (S_T)_c = \begin{cases}
           1 - D \mid \dfrac{1}{D} - p_c \mid    & \text{if } p_c \leq 2b \\
           0    & \text{otherwise}
      \end{cases}
    \end{align}
\end{minipage}

%
\paragraph{Global-Min Mean Smoothing. } We needed to develop a stable cutoff that is a meaningful estimate of the maximum distance a benign user can submit for a given round. We make use of averaging as a useful solution for reducing variability without impacting bias. Under the assumption that certain consecutive rounds have the same values of $b_c$, we can instead use the average threshold across several rounds to improve our estimate's stability. Note that we can view this average as smoothing across previous values with equal weight. Although smoothing aids against the instability problem, smoothing methods tend to exacerbate that problem resulting from $b_c$ tending to decrease rapidly over the first few communication rounds. When $b_c$ decreases rapidly, we do not wish to use any previous communication rounds for our threshold as this would result in a weaker cutoff for attacks to overcome. 

Our solution is to use as our cutoff the running mean since the lowest observed value (Global-Min Mean) of the maximum class distance for the validated user. As a result, our solution quickly adapts to decreasing values of $U = max_c (b_c)$ over communication rounds. As $U$ shrinks, we are likely to observe new global minimums, and the start of the threshold is reset. In addition, when our estimate for $U$ stabilizes, the averaging smooths our cutoff, which keeps lucky malicious users from getting past a volatile threshold. 

\begin{algorithm}[H]
\caption{ (Global-Min Mean Smoothing) \\ 
Notation: Let $x$ denote the vector of values that we wish to smooth.
}
\label{alg:smoothing}
\begin{algorithmic}[1]

    \Procedure{Global-Min Mean Smoothing}{$x_n \mid x_1, \cdots, x_{n - 1}$}
        \State compute location of global minimum over communication rounds, $i = \argmin_{j: j \in \mathopen[ n \mathclose]} x_j$
        \State subset to sequence history since and including global min, $x_i^n = \{x_i, \cdots, x_n \}$
        \State \Return average of sequence subset, $\overline{x_i^n}$
    \EndProcedure
\end{algorithmic}
\end{algorithm}

We provide Figure \ref{fig:min-mean} to compare our global min-mean smoothing with the base cutoff and various smoothing techniques. We believe that our implementation choice best captures early behavior while also providing the greatest improvements to stability. Additionally, when our threshold encounters a new global minimum, it provides a conservative estimate to prevent malicious users while re-learning cutoff behavior over the next few rounds.

\begin{figure}[H]
    \centering
    
    \includegraphics[width=.5\textwidth]{99_article/99_visuals/extra_plots/min_mean--alpha10000--alpha_val10000--d_rounds30.png}
    \caption{Comparison of Various Smoothing Methods}
    \label{fig:min-mean}
\end{figure}

\section{EXPERIMENTS}

\textcolor{red}{Expand results to CIFAR 100 and work on developing comparisons to state-of-the-ark attack and defense.}

%
\subsection{Setting}

We start by giving further specifications regarding the federated learning environment. Our interest is training a global model over M communication rounds with N users. Each iteration randomly selects K users, using a specified proportion of the total users, to participate in the model update. After local training, the next global model is the average returning model weights by the FedAvg procedure. 

To produce local datasets that do not have to be independent and identically distributed, we sample from any training data set using a Dirichlet distribution with specified parameter alpha. The Dirichlet sample determines the proportion of each class included in that user's dataset. Larger values of alpha produce more balanced class distributions. We separately control alpha for benign, malicious, and validation users. Our experiments use the CIFAR-10 dataset, which consists of 60000 32x32 color images in 10 classes, with 6000 images per class. We use the test set of 10000 images, with 500 images per class, for model evaluation. Our experiments use the ResNet18 model architecture, a state-of-the-ark off-the-shelf classifier for object recognition, initially proposed in \cite{resnet}. 

We assume that all users, including malicious, have complete control over all aspects of local training, such as learning rate, the number of epochs, and the model weights they return. The malicious users will poison a given proportion of their local data by adding their backdoor trigger to the input and changing the training label to that of the target class. They intend for the model to associate the trigger with the target class and hence have the future global model identify any input with the trigger as belonging to the target class. For simplicity, we select two main sets of training hyperparameters for benign and malicious users. 

To further show the effectiveness of our method, we choose an unrealistic and unreasonable strong attack setting. We force all malicious users to be included in the subset of users to update the global model each round after the start of the backdoor attack. Note that the selection of random users is a defense against malicious users by making it difficult for them to update the global model repeatedly. Additionally, we do not allow the validation user, a guaranteed benign user, to participate in any global model updates. We make these decisions to show the ability of our threshold to prevent even strong backdoor attacks against the global model. 

For global model evaluation, we split the test set, added the backdoor trigger to half, and removed any target class observations from that half's data. We use classification accuracy from the first half to measure model success, and the proportion of the poisoned half predicted as the target class, known as attack success rate, to measure the backdoor's extent.


%
\subsection{Main Results}

NEED TO INCLUDE COMPARISON TO SOTA DEFENSE.
\begin{figure}[H]
    \centering
    \includegraphics[width=.75\textwidth]{01_trusted/centralized/alpha10000--alpha_val10000/visuals/baseline--n_malicious1.png}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

We first demonstrate the performance of our threshold when both the trusted and other users have balanced data in Figure \ref{fig:centralized--alpha10000--alpha_val10000--accuracy--n_malicious1--m_start1}. This figure shows the classification accuracy and the attack success rate of the global model over 250 communication rounds. For this experiment, a single malicious user is included in the subset of users to potentially update the global model every single communication round. The two leftmost figures show, without defense, that the global model can be effectively attacked where the model's classification accuracy does not change. However, the right images show that the use of our defense cutoff prevents backdoor attacks and does not decrease model performance, regardless of whether a backdoor attack is present. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{01_trusted/centralized/alpha10000--alpha_val10000/visuals/accuracy--n_malicious1--m_start1.png}
    \caption{Global Model Performance For Balanced Users And Balanced Trusted User}
    \label{fig:centralized--alpha10000--alpha_val10000--accuracy--n_malicious1--m_start1}
\end{figure}

Figure \ref{fig:centralized--alpha10000--alpha_val10000--threshold--n_malicious1--m_start1--d_rounds50} shows the ability of our threshold to differentiate between benign and malicious user submission. The Threshold vs. Benign figure shows the upper and lower bounds on values considered outliers for each round. We can see our threshold closely follows what we would expect to be the largest possible value a benign user contributes. Additionally, the threshold can clearly distinguish malicious users with at least one larger class output layer distributional difference. 

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\textwidth]{01_trusted/centralized/alpha10000--alpha_val10000/visuals/threshold--n_malicious1--m_start1--d_rounds50.png}
    \caption{Threshold Performance For Balanced Users And Balanced Trusted User}
    \label{fig:centralized--alpha10000--alpha_val10000--threshold--n_malicious1--m_start1--d_rounds50}
\end{figure}

\subsection{Defense Comparison}

\subsection{Imbalanced Data}

Next, we highlight the ability of our threshold to estimate these benign users' upper bound even when the validation user has imbalanced data. As shown in Figure \ref{fig:centralized--alpha10000--alpha_val1--scaling--d_rounds50.png}, the scaled threshold corrects for the large Kolmogorov Smirnov values of imbalanced classes and gives a reasonable representation for the most considerable value that a benign user with balanced data. The model and threshold performance remain similar to the balanced validation user case; see Appendix \ref{sec.imbal_val} for actual plots. Scaling is necessary to reduce the enormous opportunity of malicious users to backdoor the global model in the early communication rounds.

\begin{figure}[H]
    \centering
    \includegraphics[width=.5\textwidth]{01_trusted/centralized/alpha10000--alpha_val1/visuals/scaling--n_malicious1--m_start1--d_rounds50.png}
    \caption{Caption}
    \label{fig:centralized--alpha10000--alpha_val1--scaling--d_rounds50.png}
\end{figure}

Our threshold is also effective against backdoor attacks when the users have imbalanced data. See Appendix \ref{sec.imbal_users} for model performance plots under this setting. Recall our earlier observation that imbalanced data is associated with more considerable class differences between the global and returning models. Therefore, in this case, the benign users will return models that fail to pass our threshold more often. Note that our threshold should not be modified for this setting as a malicious user with balanced data would have an easier time performing a backdoor attack on the global model. In summary, attempting to train a backdoor-free robust model will take additional training time when users have imbalanced data.

\subsection{Distributed Backdoor Attacks (DBA)}

We now focus on highlighting our threshold's performance under an unrealistically strong attack setting. Here we consider multiple attackers that makeup 40\% of the subset of users to update the global model every communication round. Furthermore, the backdoor trigger has been geometrically distributed between the attackers using the Distributed Backdoor Attack procedure defined in \cite{dba}. In Figure \ref{fig:distributed--alpha10000--alpha_val10000--accuracy}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{01_trusted/distributed/alpha10000--alpha_val10000/visuals/accuracy--n_malicious4--m_start1.png}
    \caption{Caption}
    \label{fig:distributed--alpha10000--alpha_val10000--accuracy}
\end{figure}

Threshold struggles with many attackers and strongly imbalanced user data - less benign updates.

% 
\section{Conclusion}

%
\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

%
\appendix

%
\section{Main Results Continued}

%
\subsection{Balanced Users, Imbalanced Validation User}
\label{sec.imbal_val}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{01_trusted/centralized/alpha10000--alpha_val1/visuals/accuracy--n_malicious1--m_start1.png}
    \caption{Global Model Performance For Balanced Users And Imbalanced Trusted User}
    \label{fig:centralized--alpha10000--alpha_val1--accuracy--n_malicious1--m_start1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\textwidth]{01_trusted/centralized/alpha10000--alpha_val1/visuals/threshold--n_malicious1--m_start1--d_rounds50.png}
    \caption{Threshold Performance For Balanced Users And Imbalanced Trusted User}
    \label{fig:centralized--alpha10000--alpha_val1--threshold--n_malicious1--m_start1--d_rounds50}
\end{figure}

%
\subsection{Balanced Users, Imbalanced Validation User}
\label{sec.imbal_users}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{01_trusted/centralized/alpha1--alpha_val1/visuals/accuracy--n_malicious1--m_start1.png}
    \caption{Global Model Performance For Imbalanced Users And Balanced Trusted User}
    \label{fig:centralized--alpha1--alpha_val10000--accuracy--n_malicious1--m_start1}
\end{figure} 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{01_trusted/centralized/alpha1--alpha_val1/visuals/accuracy--n_malicious1--m_start1.png}
    \caption{Global Model Performance For Imbalanced Users And Imbalanced Trusted User}
    \label{fig:centralized--alpha1--alpha_val1--accuracy--n_malicious1--m_start1}
\end{figure} 



% 
\section{Retired Text (Saved If Needed Later)}

%
\subsection{Warm-up Period}

Our final threshold includes a warm-up period, global min-mean smoothing, and class frequency scaling to address the above concerns, respectively.

If we make the additional assumption of independence between classes distributions, we can show, see Equation \ref{eq:leq_bound} that alternative scaling of the maximum distance can instead produce a estimator that is conservative, on average. Furthermore, Equation \ref{eq:leq_bound} holds with equality instead when class distances share identical supports (ie. $\forall c \in \mathopen[ D \mathclose], b_c = b$). Note that the conservative scaling $(D + 1) / D \in \mathopen(1, \mathclose 1.5]$, and that larger classification problems require smaller initial scaling. This agrees with intuition that larger-way classification problems may require longer warm-up periods. 

\begin{align}
    & \textrm{Suppose } \forall c \in \mathopen[ D \mathclose], (KS)_c \overset{\indep}{\sim} Uniform(0, b_c) \nonumber \\
    & \quad \dfrac{(KS)_c}{b_c} \overset{\mathrm{iid}}{\sim} Uniform(0, 1) \nonumber \\
    & \quad \max_c \left( \dfrac{(KS)_c}{b_c} \right) \sim Beta(D, 1) \nonumber \\
    & \textrm{Let } j = \argmax_c \left( b_c \right) \textrm{ such that } b_j = \max_c \left( b_c \right) \nonumber \\
    & \quad \max_c \left( \dfrac{(KS)_c}{b_j} \right) \leq \max_c \left( \dfrac{(KS)_c}{b_c} \right) \nonumber \\
    & \quad E \left[ \max_c \left( \dfrac{(KS)_c}{b_j} \right) \right] \leq E \left[ \max_c \left( \dfrac{(KS)_c}{b_c} \right) \right] = \dfrac{D}{D + 1} \nonumber \\
    & \quad E \left[ \left( \dfrac{D + 1}{D} \right) \max_c \left( (KS)_c \right) \right] \leq b_j \label{eq:leq_bound}
\end{align}

We recommend traversing coefficients from $(D + 1) / D$ to 2 to assist in dampening the large opportunity for malicious users to impact the global model at the beginning of Federated Learning. It may be often more practical, additional training time permitting, to start with scaling by 1 to avoid fine-tuning this procedure for large classification problems. Additionally, we believe that the length and intermediate values chosen, should be greatly influenced by the context of the application. Longer warm-up periods may be most useful when there is greater chance or consequence of attack.


\subsection{Computation}
Only the validation user's local training, requiring gradient calculations, is an additional step with a high relative computational cost. However, users often train on local compute resources in the Federated Learning setting. Therefore, the validation user will do this step simultaneously with the local training of other users, largely avoiding extra training time. The only other steps not already done during Federated Learning are forward passes, which are less complex and quickly accelerated by modern computing architectures. \yli{We can discuss the advantage of the method at the end of sec 3. Let's focus on describing the method first.}

We make use of the empirical CDF function to avoid distributional assumptions on the output layers and KS distances for computational efficiency. Using the property that the empirical CDFs are step functions, note that the KS distance between the empirical CDFs only needs to be computed for each of the values of the class scores on the local user's data for both the global and trusted user's updated model, which is easily computed in-parallel.

%
\subsection{Entropy-Based Scaling}
From information theory, Shannon Entropy is one such measure of diversity. Note that entropy is maximized when class labels are uniformly distributed. 

We propose scaling the KS distances between global and updated models for each class $(KS)_c$ by the following factor $S_c$, before computing the maximum and multiplying by two. Note that $S$, the collection of $S_c$, only needs to be computed once, as it depends on the local user's class label distribution and does not depend on the distance values each round.

\begin{definition}[Shannon Entropy] For a discrete random variable Y, taking values $v$, with probability mass function $p(y)$, Shannon Entropy is defined as $H(Y) = - \sum_{v} p(v) \log p(v)$.
    
\end{definition}

\begin{align}
    y_c &= \min \left( x_c, \dfrac{1}{e} \right) \\
    z_c &= \min \left( \dfrac{P(y_c) \ln \left( P(y_c) \right)}{\dfrac{1}{D} \ln \left( \dfrac{1}{D} \right)}, 2 \right) \\
    S_c &= 1 - | 1 - z_c |    
\end{align}

% figure omitted
See Figure \ref{fig:scaling} for $S_c(x)$ plotted as a function of the class proportions for several values of $D$, the total number of classes. Observe the following  properties:
\begin{enumerate}
    \item Scaling is between 0 and 1, and there is no scaling when a given class is balanced
    \item Under-represented classes are scaled down to zero
    \item For many-class classification problems, we are less trusting of an over-represented class as we have more potential classes for at least one to be approximately balanced.
\end{enumerate}

We believe further research is needed regarding extremely imbalanced edge cases. However, we propose this scaling as a remedy for reasonably imbalanced data, demonstrating its promising effectiveness in our experimentation.


\end{document}
